{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking NeuralNet.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in required files for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include(\"src/algorithms/nn.jl\");\n",
    "include(\"src/utils/parse.jl\");\n",
    "\n",
    "using Gadfly;\n",
    "using CategoricalArrays;\n",
    "using DataFrames;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's benchmark the autoencoder sum of squared error as a function of the learning rate and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = [0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001];\n",
    "epochs = [1000, 5000, 10000, 50000, 100000, 500000];\n",
    "T = 5;\n",
    "\n",
    "results = zeros(6, 6);\n",
    "\n",
    "for alpha in 1:length(lr)\n",
    "    for N in 1:length(epochs)\n",
    "        println(string(lr[alpha], \"\\t\", epochs[N]))\n",
    "        res = 0.0;\n",
    "        for i in 1:T\n",
    "            sserr = encode_8x3x8(epochs[N], lr[alpha], \"\"; write_output=false, verbose=false)[4];\n",
    "            res += sserr\n",
    "        end\n",
    "        results[N, alpha] = res / T;\n",
    "\n",
    "    end\n",
    "end\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultdf = DataFrame(sserr = results[1,:], lr = lr, N=epochs[1])\n",
    "resultdf = vcat(resultdf, DataFrame(sserr = results[2,:], lr=lr, N=epochs[2]))\n",
    "resultdf = vcat(resultdf, DataFrame(sserr = results[3,:], lr=lr, N=epochs[3]))\n",
    "resultdf = vcat(resultdf, DataFrame(sserr = results[4,:], lr=lr, N=epochs[4]))\n",
    "resultdf = vcat(resultdf, DataFrame(sserr = results[5,:], lr=lr, N=epochs[5]))\n",
    "resultdf = vcat(resultdf, DataFrame(sserr = results[6,:], lr=lr, N=epochs[6]))\n",
    "\n",
    "plot(resultdf, y=\"sserr\", x=\"lr\", color=\"N\", Geom.line, Guide.ylabel(\"Sum Sq. Error\"), Guide.xlabel(\"Learning Rate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using the optimal parameters we found in the previous section, let's look at the hidden layer's representation of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "N = 100000\n",
    "h_out = encode_8x3x8(N, alpha, \"\"; write_output=false, verbose=false)[3];\n",
    "h_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's turn to the neural net where we will benchmark the size of the hidden layer first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_seq_fp = \"data/rap1-lieb-positives.txt\";\n",
    "total_seq_fp = \"data/yeast-upstream-1k-negative.fa\";\n",
    "test_fp = \"data/rap1-lieb-test.txt\";\n",
    "\n",
    "tdata, labels = parse_input(pos_seq_fp, total_seq_fp; balance=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_sizes = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12];\n",
    "T = 5\n",
    "preds = zeros(length(hl_sizes))\n",
    "\n",
    "for hl_size in 1:length(hl_sizes)\n",
    "    pred = 0.0\n",
    "    println(hl_sizes[hl_size]);\n",
    "    for i in 1:T\n",
    "        pred += nn_3layer(tdata, labels, hl_size, 1000, 0.05, \"\")[5]\n",
    "    end\n",
    "    preds[hl_size] = pred / T\n",
    "end\n",
    "    \n",
    "plot(x=hl_sizes, y=preds, Guide.xlabel(\"Hidden Layer Size\"), Guide.ylabel(\"Average Predictive Ability\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's evaulate the effect of training set bias on our ability to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "biases = [1, 2, 3, 4, 5]\n",
    "T = 5\n",
    "preds = zeros(length(biases))\n",
    "\n",
    "for bias in 1:length(biases)\n",
    "    pred = 0.0\n",
    "    println(biases[bias])\n",
    "    for i in 1:T\n",
    "        tdata, labels = parse_input(pos_seq_fp, total_seq_fp; balance=biases[bias]);\n",
    "        pred += nn_3layer(tdata, labels, 11, 3000, 0.05, \"\")[5]\n",
    "    end\n",
    "    preds[bias] = pred / T\n",
    "end\n",
    "\n",
    "plot(x=biases, y=preds, Guide.xlabel(\"Bias Factor\"), Guide.ylabel(\"Average Predictive Ability\"), Geom.line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Finally, let's find the optimal set of learning parameters using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = [1000, 3000, 5000];\n",
    "alphas = [0.05, 0.01, 0.005, 0.001];\n",
    "\n",
    "results = zeros(3, 4);\n",
    "\n",
    "for i in 1:length(Ns)\n",
    "    for j in 1:length(alphas)\n",
    "        println(\"$(Ns[i]), $(alphas[j])\")\n",
    "        fpred = train_nn(tdata, labels, 11, Ns[i], alphas[j], \"\"; write_output = false, verbose = false, C=10);\n",
    "        results[i, j] = fpred\n",
    "    end\n",
    "end\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultdf = DataFrame(pred = results[:,1], lr = alphas[1], N=Ns)\n",
    "resultdf = vcat(resultdf, DataFrame(pred = results[:,2], lr=alphas[2], N=Ns))\n",
    "resultdf = vcat(resultdf, DataFrame(pred = results[:,3], lr=alphas[3], N=Ns))\n",
    "resultdf = vcat(resultdf, DataFrame(pred = results[:,4], lr=alphas[4], N=Ns))\n",
    "\n",
    "\n",
    "plot(resultdf, y=\"pred\", x=\"N\", color=\"lr\", Geom.line, Guide.ylabel(\"Average Predictive Value\"), Guide.xlabel(\"Number of Epochs\"),\n",
    "    Coord.cartesian(xmin=1000, xmax=5000), Guide.colorkey(title=\"Learning Rate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the plot above, it's apparent that  the best combination of parameters is around $N = 5000$ and $\\alpha = 0.05$. Now let's look at how sensitive these parameters are to class inbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biases = [1, 2, 3, 4, 5]\n",
    "preds = zeros(length(biases))\n",
    "\n",
    "N = 5000\n",
    "alpha = 0.05\n",
    "\n",
    "for bias in 1:length(biases)\n",
    "    println(biases[bias])\n",
    "    tdata, labels = parse_input(pos_seq_fp, total_seq_fp; balance=biases[bias]);\n",
    "    preds[bias] = train_nn(tdata, labels, 11, N, alpha, \"\"; write_output = false, verbose = false, C=10);\n",
    "end\n",
    "\n",
    "plot(x=biases, y=preds, Guide.xlabel(\"Bias Factor\"), Guide.ylabel(\"Average Predictive Ability\"), Geom.line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we've benchmarked and selected parameters, we can predict on the test data set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata, labels = parse_training(pos_seq_fp, total_seq_fp; balance=5);\n",
    "test_data = parse_testing(test_fp)\n",
    "\n",
    "final_accur = nn_predict_on_data(tdata, labels, test_data, 11, 5000, 0.05, \"final_predictions.txt\"; C = 20);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.1",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
